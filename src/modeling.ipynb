{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from preprocessing import run_preprocessing, load_preprocessed_data\n",
    "from models import NeuralNet, CustomDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    X = df.drop(\"prec\", axis=1)\n",
    "    y = df[\"prec\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1424400, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_preprocessed_data()\n",
    "X_train, X_test, y_train, y_test = split_data(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural Net__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss: 3.4746498604334977\n",
      "Epoch 0, Validation loss: 3.179063055050021\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_1.pth\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 3.1406764870124544\n",
      "Epoch 1, Validation loss: 3.060097099493163\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_2.pth\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss: 3.0583393586997962\n",
      "Epoch 2, Validation loss: 2.9983593753949243\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_3.pth\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training loss: 3.0070596440459707\n",
      "Epoch 3, Validation loss: 2.9589551318204204\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_4.pth\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training loss: 2.9689472406987814\n",
      "Epoch 4, Validation loss: 2.9270779341303053\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_5.pth\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training loss: 2.939197319233372\n",
      "Epoch 5, Validation loss: 2.8995880791397175\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_6.pth\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training loss: 2.9147295261684216\n",
      "Epoch 6, Validation loss: 2.8773301629154595\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_7.pth\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training loss: 2.8938947009910208\n",
      "Epoch 7, Validation loss: 2.861330593151837\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_8.pth\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training loss: 2.8748790867980545\n",
      "Epoch 8, Validation loss: 2.8467484589410037\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_9.pth\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training loss: 2.8581615967951017\n",
      "Epoch 9, Validation loss: 2.833800912867482\n",
      "Model checkpoint saved at c:\\Users\\Alexander Lorenz\\Documents\\GitHub\\LamaH-Precipitation-Forecast\\model\\checkpoints/model_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "# create data loader\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# create model\n",
    "model = NeuralNet()\n",
    "\n",
    "# train model\n",
    "model.fit(num_epochs=10, trainloader=trainloader, testloader=testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set RMSE: 2.8338\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(dataloader=testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparemeter Tuning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(24, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "  \n",
    "  \n",
    "  def fit(self, num_epochs, trainloader, testloader, checkpoint_dir=\"../model/checkpoints\"):\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        self.train()\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        current_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(tqdm(trainloader, desc=f'Epoch {epoch+1}', leave=False), 0):\n",
    "            # prepare\n",
    "            X, y_true = data\n",
    "\n",
    "            # forward: predict\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            # compute loss\n",
    "            loss = torch.sqrt(loss_function(y_pred, y_true))\n",
    "\n",
    "            # backpropagating gradient of loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters (weights and bias)\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "\n",
    "        print(\"Epoch {}, Training loss: {}\".format(epoch, current_loss / len(trainloader)))\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            with torch.no_grad():\n",
    "                X, y_true = data\n",
    "                y_pred = self.forward(X)\n",
    "                loss = torch.sqrt(loss_function(y_pred, y_true))  # RMSE\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        train.report(loss=(val_loss / val_steps))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 11:40:29,388\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown argument found in the Trainable function. The function args must include a single 'config' positional parameter.\nFound: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 19\u001b[0m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m)]),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m)]),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[0;32m     12\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Alexander Lorenz\\anaconda3\\envs\\ml\\Lib\\site-packages\\ray\\tune\\tune.py:758\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(experiments):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exp, Experiment):\n\u001b[1;32m--> 758\u001b[0m         experiments[i] \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_budget_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_budget_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstorage_filesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_filesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m            \u001b[49m\u001b[43msync_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial_name_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_name_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexport_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fail_fast \u001b[38;5;129;01mand\u001b[39;00m max_failures \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_failures must be 0 if fail_fast=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alexander Lorenz\\anaconda3\\envs\\ml\\Lib\\site-packages\\ray\\tune\\experiment\\experiment.py:149\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[1;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, sync_config, checkpoint_config, trial_name_creator, trial_dirname_creator, log_to_file, export_formats, max_failures, restore, local_dir)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be set for a function trainable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will need to report a checkpoint every \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this behavior.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_identifier \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mrpc_code \u001b[38;5;241m==\u001b[39m ray\u001b[38;5;241m.\u001b[39m_raylet\u001b[38;5;241m.\u001b[39mGRPC_STATUS_CODE_RESOURCE_EXHAUSTED:\n",
      "File \u001b[1;32mc:\\Users\\Alexander Lorenz\\anaconda3\\envs\\ml\\Lib\\site-packages\\ray\\tune\\experiment\\experiment.py:351\u001b[0m, in \u001b[0;36mExperiment.register_if_needed\u001b[1;34m(cls, run_object)\u001b[0m\n\u001b[0;32m    349\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_trainable_name(run_object)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[43mregister_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, PicklingError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    353\u001b[0m     extra_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-Try reproducing the issue by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe type annotations and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Alexander Lorenz\\anaconda3\\envs\\ml\\Lib\\site-packages\\ray\\tune\\registry.py:110\u001b[0m, in \u001b[0;36mregister_trainable\u001b[1;34m(name, trainable, warn)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainable, FunctionType) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainable, partial):\n\u001b[0;32m    109\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected function for trainable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m     trainable \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(trainable):\n\u001b[0;32m    112\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected unknown callable for trainable. Converting to class.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alexander Lorenz\\anaconda3\\envs\\ml\\Lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py:212\u001b[0m, in \u001b[0;36mwrap_function\u001b[1;34m(train_func, name)\u001b[0m\n\u001b[0;32m    209\u001b[0m use_config_single \u001b[38;5;241m=\u001b[39m _detect_config_single(train_func)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_config_single:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown argument found in the Trainable function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe function args must include a single \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m positional parameter.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(func_args)\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    218\u001b[0m resources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(train_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImplicitFunc\u001b[39;00m(\u001b[38;5;241m*\u001b[39minherit_from):\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown argument found in the Trainable function. The function args must include a single 'config' positional parameter.\nFound: []"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model.fit(num_epochs=1, trainloader=trainloader, testloader=testloader)\n",
    "\n",
    "config = {\n",
    "    \"l1\": tune.choice([2 ** i for i in range(7, 10)]),\n",
    "    \"l2\": tune.choice([2 ** i for i in range(7, 10)]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4])\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    train_model,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "print(\"Best config: \", result.get_best_config(metric=\"loss\", mode=\"min\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
